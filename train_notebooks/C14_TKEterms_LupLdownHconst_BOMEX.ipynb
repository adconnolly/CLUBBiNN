{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wandb\n",
    "# import wandb\n",
    "\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.backends import mps\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Our fancy new modules\n",
    "import subgrid_parameterization.arch.ann as ann\n",
    "import subgrid_parameterization.preprocess.SAM_helpers as sam\n",
    "from subgrid_parameterization.preprocess.torch_helpers import split_dataset\n",
    "from subgrid_parameterization.util.earlystopper import EarlyStopper\n",
    "import subgrid_parameterization.util.plot_helpers as plot_helpers\n",
    "\n",
    "from subgrid_parameterization.preprocess.C14 import get_C14\n",
    "from subgrid_parameterization.preprocess.mixing_length import get_mixing_length\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix seeds for reproducibility\n",
    "seed = 12345\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.mps.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path0 = \"/Users/jwa34/CLUBBED/\"\n",
    "\n",
    "path = \"test/data/\"\n",
    "file = \"BOMEX_64x64x75_100m_40m_1s\"\n",
    "save = \"C14profiles_BOMEX\"\n",
    "\n",
    "ds = xr.open_dataset(path0 + path + file + \".nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nzm, nzt, ngrdcol, zm, zt, dzm, dzt, invrs_dzm, invrs_dzt = sam.get_grid(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L, Lup, Ldown = get_mixing_length(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hscale = 1000  # 1km to normalize mixing lengths which are bounded by  ~40 m and ~3km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C14 = get_C14(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up2 = sam.stagger_var(\"U2\", ds)\n",
    "vp2 = sam.stagger_var(\"V2\", ds)\n",
    "wp2 = sam.stagger_var(\"W2\", ds)\n",
    "e = 0.5 * (up2 + vp2 + wp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = sam.get_disp(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C14min = 0.2\n",
    "C14max = 2\n",
    "minMask = disp < -2 / 3 * C14min / L * e**1.5\n",
    "maxMask = e > (-1.5 * disp * L / C14max) ** (2 / 3)\n",
    "# maxMask = e[minMask] >  (-1.5*disp[minMask]*L[minMask]/C14max)**(2/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = list()\n",
    "output = list()\n",
    "for it in range(ngrdcol):\n",
    "    for k in range(nzt):\n",
    "        if minMask[it, k] and maxMask[it, k]:\n",
    "            input.append(\n",
    "                [\n",
    "                    up2[it, k] / e[it, k],\n",
    "                    vp2[it, k] / e[it, k],\n",
    "                    wp2[it, k] / e[it, k],\n",
    "                    Lup[it, k] / Hscale,\n",
    "                    Ldown[it, k] / Hscale,\n",
    "                ]\n",
    "            )\n",
    "            output.append([C14[it, k]])\n",
    "print(str(len(input)) + \" samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bomex_dataset = Data.TensorDataset(torch.tensor(input), torch.tensor(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = split_dataset(\n",
    "    bomex_dataset, 0.2\n",
    ")  # random_split(bomex_dataset,[0.8,0.2]) returns a Subset so doesn't work later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, C14train = train_dataset.tensors\n",
    "# np.array([numerator/(np.std(np.array(output)[:,i])) for i in range(np.array(output).shape[1])])\n",
    "lossweights = np.ones(C14train.detach().numpy().shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_size\": 360,\n",
    "    \"lr\": 0.001,  ## learning rate\n",
    "    \"wd\": 0.01,  ## weight decay\n",
    "    \"epochs\": 2000,  ## Setting this to a high number because early stopping\n",
    "    # \"subsample\":10,   ## Take a subsample of 1000 data points\n",
    "    \"patience\": 20,  ## Patience for early stopping\n",
    "}\n",
    "\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use GPUs if available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA Available\")\n",
    "    device = torch.device(\"cuda\")\n",
    "# elif mps.is_available():\n",
    "#    print(\"MPS Available\")\n",
    "#    device = torch.device(\"mps\")\n",
    "else:\n",
    "    print(\"CUDA Not Available\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Construct training and validation dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "valid_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "## Record number of training/validation images\n",
    "# config[\"training_fields\"]=len(pyqg_dataset.test_idx)\n",
    "# config[\"validation_fields\"]=len(pyqg_dataset.valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvars = 5  # ups,vp2,wp2,Lup,Ldown\n",
    "iso = True\n",
    "if iso:\n",
    "    nvarsout = 1\n",
    "else:\n",
    "    nvarsout = 2\n",
    "\n",
    "N = [nvars, 16, 8, nvarsout]\n",
    "\n",
    "try:\n",
    "    del model\n",
    "    model = ann.ANN(N).double()  ## NN architecture: could be FCNN, resnet, ANN.. etc\n",
    "except:\n",
    "    model = ann.ANN(N).double()  ## NN architecture: could be FCNN, resnet, ANN.. etc\n",
    "\n",
    "model.to(device)\n",
    "config[\"learnable parameters\"] = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config[\"lr\"],\n",
    ")  # weight_decay=config[\"wd\"], betas=(beta1, beta2))\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3, patience=10)\n",
    "criterion = nn.MSELoss()\n",
    "early_stopper = EarlyStopper(patience=config[\"patience\"], min_delta=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(project=\"wandb_demo\", entity=\"m2lines\",config=config)\n",
    "# wandb.watch(model, log_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = list()\n",
    "test_loss = list()\n",
    "weights = torch.from_numpy(lossweights).to(device)\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    train_samples = 0\n",
    "    train_running_loss = 0.0\n",
    "    valid_running_loss = 0.0\n",
    "    valid_samples = 0\n",
    "\n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        x_data, y_data = data\n",
    "        x_data = x_data.to(device)\n",
    "        y_data = y_data.to(device)\n",
    "\n",
    "        ## zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(x_data)  ## Takes in Q, outputs \\hat{S}\n",
    "        loss = criterion(output * weights, y_data * weights)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ## Store loss values\n",
    "        train_running_loss += loss.detach() * x_data.shape[0]\n",
    "        train_samples += x_data.shape[0]\n",
    "    train_running_loss /= train_samples\n",
    "\n",
    "    model.eval()\n",
    "    for data in valid_loader:\n",
    "        x_data, y_data = data\n",
    "        x_data = x_data.to(device)\n",
    "        y_data = y_data.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_data)  ## Takes in Q, outputs \\hat{S}\n",
    "        val_loss = criterion(output, y_data)\n",
    "        ## Store loss values\n",
    "        valid_running_loss += val_loss.detach() * x_data.shape[0]\n",
    "        valid_samples += x_data.shape[0]\n",
    "    valid_running_loss /= valid_samples\n",
    "\n",
    "    early_stop = early_stopper.early_stop(valid_running_loss)\n",
    "    if early_stop:\n",
    "        print(\"Early stopping epoch: \" + str(epoch - early_stopper.patience))  # +/- 1?\n",
    "        break\n",
    "    else:\n",
    "        torch.save(model.state_dict(), save + \"_net_before.pt\")\n",
    "        torch.save(optimizer.state_dict(), save + \"_optim_before.pt\")\n",
    "\n",
    "    # ## Push loss values for each epoch to wandb\n",
    "    log_dic = {}\n",
    "    log_dic[\"epoch\"] = epoch\n",
    "    log_dic[\"training_loss\"] = (train_running_loss / train_samples).cpu().numpy()\n",
    "    log_dic[\"valid_loss\"] = (valid_running_loss / valid_samples).cpu().numpy()\n",
    "    # wandb.log(log_dic)\n",
    "    train_loss.append((train_running_loss / train_samples).cpu().numpy())\n",
    "    test_loss.append((valid_running_loss / valid_samples).cpu().numpy())\n",
    "\n",
    "    # verbose\n",
    "    print(\n",
    "        \"%03d %.3e %.3e \"\n",
    "        % (log_dic[\"epoch\"], log_dic[\"training_loss\"], log_dic[\"valid_loss\"]),\n",
    "        end=\"\",\n",
    "    )\n",
    "    print(\"\")\n",
    "model.load_state_dict(\n",
    "    torch.load(save + \"_net_before.pt\")\n",
    ")  # ,map_location=device),strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutStartPct = 0.5\n",
    "zoomEndEpochs = 2 * early_stopper.patience\n",
    "plot_helpers.plot_losses(\n",
    "    [test_loss, train_loss], [\"Test\", \"Train\"], cutStartPct, zoomEndEpochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = test_dataset.tensors\n",
    "# x_test=x_test.to(device)\n",
    "y_test = y_test.squeeze().detach().cpu().numpy()\n",
    "y_pred = model(x_test).squeeze().detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_text = r\"$C_{14}$\"\n",
    "# r2=np.empty(nzt)\n",
    "# r=np.empty(nzt)\n",
    "# for k in range(nzt):\n",
    "#     r2[k]=r2_score(y_test[:,k], y_pred[:,k])\n",
    "#     r[k]=np.corrcoef(y_test[:,k], y_pred[:,k])[0, 1]\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "r = np.corrcoef(y_test, y_pred)[0, 1]\n",
    "\n",
    "print(\"Statistics for \" + y_text)\n",
    "print(\"R^2: %.4f\" % np.mean(r2))\n",
    "print(\"Correlation: %.4f\" % +np.mean(r) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = \"Predicted\"\n",
    "l2 = \"True\"\n",
    "nvar = 1\n",
    "# z=zt[0,:]\n",
    "y_text = r\"$C_{14}$\"\n",
    "\n",
    "# fig1,ax1 = plt.subplots(1,nvar,figsize = (12, 6))\n",
    "fig2, ax2 = plt.subplots(1, nvar, figsize=(12, 6))\n",
    "# fig3,ax3 = plt.subplots(1,nvar,figsize = (12, 6))\n",
    "\n",
    "ax2.scatter(y_test, y_pred)\n",
    "xmin, xmax = ax2.get_xlim()\n",
    "ymin, ymax = ax2.get_ylim()\n",
    "ax2.plot([xmin, xmax], [xmin, xmax])\n",
    "ax2.set_xlim([xmin, xmax])\n",
    "ax2.set_xlabel(l2)\n",
    "ax2.set_ylabel(l1)\n",
    "ax2.set_title(y_text, fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax1.plot(r2,z,'b',linewidth=2)\n",
    "# ax1.set_title(y_text,fontsize=20)\n",
    "# ax1.set_xlabel(r'R$^2$',fontsize=20)\n",
    "# ax1.set_ylabel(r'$\\frac{z}{z_{\\text{top}}}$',rotation=0,fontsize=20)\n",
    "# print(\"Avg. across all levels for \"+y_text)\n",
    "# print(\"R^2: %.4f\" % np.mean(r2) )\n",
    "# print(\"Correlation: %.4f\" % +np.mean(r)+\"\\n\")\n",
    "\n",
    "# for it in range(y_pred.shape[0]):\n",
    "#     ax3.plot(y_pred[it],z,'r',label=l1)\n",
    "#     ax3.plot(y_test[it],z,'k',label=l2)\n",
    "#     l1,l2='__nolegend__','__nolegend__'\n",
    "#     ax3.set_xlabel(y_text,fontsize=20)\n",
    "#     ax3.set_ylabel(r'$z$',rotation=0,fontsize=20)\n",
    "\n",
    "# # ax3.set_ylabel(r'$\\frac{z}{z_{\\text{top}}}$',rotation=0,fontsize=20)\n",
    "# ax3[0].legend()\n",
    "# l1='Predicted'\n",
    "# l2='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_text=[r\"$\\overline{u'w'}/u_*^2$\",r\"$\\overline{v'w'}/u_*^2$\"]\n",
    "# y_text=[\"U2DFSN\",\"V2DFSN\"]\n",
    "# y_text[r'C_{14}']\n",
    "# fig1,fig2,fig3=quickPlots(model,test_dataset,y_text,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to(\"cpu\")\n",
    "# figure_fields=wandb.Image(plot_helpers.plot_fields(pyqg_dataset,model))\n",
    "# wandb.log({\"Fields\": figure_fields})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2_upper,r2_lower,figure_power=metrics.get_offline_metrics(model,valid_loader)\n",
    "# figure_power=wandb.Image(figure_power)\n",
    "# wandb.log({\"Power spectrum\": figure_power})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.run.summary[\"r2_upper\"]=r2_upper\n",
    "# wandb.run.summary[\"r2_lower\"]=r2_lower\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "\n",
    "def file_checksum(path):\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "print(f\"Net checksum:   {file_checksum(save + '_net_before.pt')}\")\n",
    "print(f\"Optim checksum: {file_checksum(save + '_optim_before.pt')}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
